{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T10:53:40.883893Z",
     "start_time": "2023-10-30T10:53:29.116040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2187858  -0.05383734] 0.026941580756013744\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    (a, b) = tx.T @ tx, tx.T @ y\n",
    "    ws = np.linalg.lstsq(a, b,rcond=None)\n",
    "    w = ws[0]\n",
    "    # Calculate residuals\n",
    "    res = y - tx @ w\n",
    "    # Calculate the Mean Squared Error\n",
    "    mse = (res.T @ res)/(2*len(y))\n",
    "    return w, mse\n",
    "\n",
    "test_y = np.array([0.1, 0.3, 0.5])\n",
    "test_x = np.array([[2.3, 3.2], [1.0, 0.1], [1.4, 2.3]])\n",
    "test_res_w, test_res_mse = least_squares(test_y, test_x)\n",
    "print(test_res_w, test_res_mse)\n",
    "np.testing.assert_allclose(test_res_w, np.array([0.218786, -0.053837]), rtol=1e-4, atol=1e-8)\n",
    "np.testing.assert_allclose(test_res_mse, 0.026942, rtol=1e-4, atol=1e-8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T10:54:33.261619Z",
     "start_time": "2023-10-30T10:54:33.193585Z"
    }
   },
   "id": "3ff41602bce83d3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data/dataset_to_release\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:37.720247Z",
     "start_time": "2023-10-30T10:56:55.694175Z"
    }
   },
   "id": "31ed296dab042f3"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "((328135, 321),\n (328135,),\n array([5.30000000e+01, 1.10000000e+01, 1.11620150e+07, 1.10000000e+01,\n        1.60000000e+01, 2.01500000e+03, 1.10000000e+03, 2.01501563e+09,\n        2.01501563e+09, 1.00000000e+00, 1.00016956e+00, 1.00000000e+00,\n        1.00000000e+00, 1.54463226e+00, 1.65625000e+00, 1.79388666e+00,\n        8.01570428e-01, 9.91935740e-01, 1.00000000e+00, 1.00000000e+00,\n        2.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 2.00000000e+00, 2.00000000e+00, 1.00000000e+00,\n        5.00000000e+00, 8.80000000e+01, 1.00000000e+00, 1.00000000e+00,\n        2.00000000e+00, 1.00000000e+00, 3.00000000e+00, 1.17256318e+00,\n        1.00000000e+00, 1.00000000e+00, 2.00000000e+00, 2.00000000e+00,\n        2.00000000e+00, 1.48319767e+00, 2.00000000e+00, 2.00000000e+00,\n        2.00000000e+00, 2.00000000e+00, 1.00000000e+00, 2.00000000e+00,\n        3.00000000e+00, 5.44746802e+01, 2.00000000e+00, 1.00000000e+00,\n        5.00000000e+00, 1.00000000e+00, 1.97233997e+00, 1.72032356e+00,\n        1.22575244e+00, 2.00000000e+00, 1.00000000e+00, 8.80000000e+01,\n        8.00000000e+00, 1.00000000e+00, 1.10000000e+02, 5.01000000e+02,\n        2.00913930e+00, 1.00000000e+00, 2.00000000e+00, 2.00000000e+00,\n        2.00000000e+00, 2.00000000e+00, 2.00000000e+00, 2.00000000e+00,\n        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 6.75296917e+00,\n        3.00000000e+00, 8.88000000e+02, 3.46937270e+00, 6.77992327e+01,\n        5.56346536e+00, 5.55000000e+02, 2.05000000e+02, 3.04000000e+02,\n        3.03000000e+02, 3.08000000e+02, 2.05000000e+02, 1.00000000e+00,\n        6.40000000e+01, 1.02000000e+02, 3.00000000e+01, 9.80000000e+01,\n        2.20000000e+02, 3.00000000e+01, 1.05000000e+02, 1.57779468e+00,\n        1.86260040e+00, 2.44153837e+00, 6.18715357e+00, 1.00000000e+00,\n        1.00000000e+00, 1.02015000e+05, 8.00000000e+00, 2.00000000e+00,\n        2.00000000e+00, 3.88963865e+05, 4.59039421e+00, 1.61755037e+00,\n        2.79715523e+00, 1.67853524e+00, 2.53753927e+02, 2.68753858e+02,\n        1.45231733e+01, 1.44926748e+01, 2.42425835e+01, 2.41062379e+00,\n        1.88523382e+00, 1.46606773e+00, 1.80866198e+00, 7.57421634e+00,\n        3.36031434e+00, 2.21024228e+00, 1.26869653e+01, 1.53390069e+00,\n        1.26037672e+00, 5.51178965e+00, 2.21510480e+00, 1.36198904e+00,\n        1.72527473e+00, 2.40025359e+00, 6.03894472e+00, 2.83858899e+00,\n        1.58855932e+00, 2.43129771e+00, 1.95547074e+00, 1.99957537e+00,\n        1.94221048e+00, 3.98125906e+00, 4.06976985e+00, 2.20525292e+00,\n        3.99989616e+00, 1.60498960e+00, 1.41405102e+00, 4.25985824e+02,\n        1.72868724e+00, 6.29257362e+01, 1.62653061e+00, 6.06929825e+01,\n        5.42149123e+01, 3.67946612e+01, 6.80484600e+02, 4.24279835e+00,\n        5.32163743e+00, 4.41855670e+00, 4.92783505e+00, 1.72445019e+00,\n        1.64796905e+00, 1.70916987e+00, 2.85595371e+00, 1.81683759e+00,\n        1.31187411e+00, 1.71404722e+00, 2.23873843e+00, 1.67965695e+00,\n        1.49474985e+00, 1.89706791e+00, 3.42261806e+00, 2.35331932e+00,\n        1.44431301e+01, 1.80013462e+00, 1.00000000e+00, 3.00000000e+00,\n        1.00000000e+00, 5.00000000e+00, 2.00000000e+00, 2.54966249e+00,\n        1.00000000e+00, 1.13598673e+00, 1.84437264e+00, 1.00000000e+00,\n        4.00000000e+00, 1.00000000e+00, 2.00000000e+00, 4.00000000e+00,\n        1.63890029e+00, 1.96199671e+00, 1.68764137e+00, 1.68414559e+00,\n        1.90416667e+00, 1.69131589e+00, 2.31543624e+00, 1.79329609e+00,\n        4.12308706e+00, 4.33151047e+00, 1.89793640e+00, 4.37364326e+01,\n        1.86888186e+00, 4.74660632e+01, 1.25401937e+00, 4.05103709e+00,\n        1.84126877e+00, 1.70920436e+00, 2.19041162e+00, 1.46331324e+00,\n        1.82333046e+00, 1.67031654e+00, 6.35791559e+01, 6.69482417e+01,\n        5.13166457e+01, 3.93667394e+01, 5.99864310e+01, 7.40668741e+01,\n        7.60298191e+01, 7.89925729e+01, 1.87782055e+00, 1.87344798e+00,\n        2.00000000e+01, 1.00000000e+00, 2.54225970e+00, 5.32049000e+05,\n        1.20931680e+02, 1.00000000e+00, 1.20931680e+02, 7.60709132e+00,\n        7.22264058e+00, 7.22264058e+00, 8.05383803e+02, 2.00000000e+00,\n        2.33947890e-01, 2.64741181e+02, 1.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 3.00000000e+00, 2.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 1.00000000e+00, 8.00000000e+00, 1.00000000e+00,\n        5.70000000e+01, 5.00000000e+00, 6.10000000e+01, 1.55000000e+00,\n        4.99000000e+01, 2.07800000e+01, 2.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 3.00000000e+00, 5.00000000e+00, 1.00000000e+00,\n        2.00000000e+00, 2.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 7.10000000e-01,\n        1.30000000e-01, 1.00000000e-01, 2.70000000e-01, 7.10000000e-01,\n        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        7.10000000e-01, 1.21000000e+00, 2.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        1.00000000e+00, 3.50000000e+00, 4.50000000e+00, 2.69100000e+01,\n        4.61000000e+00, 1.00000000e+00, 1.00000000e+00, 3.00000000e+01,\n        3.00000000e+01, 2.00000000e+00, 4.66700000e+00, 6.00000000e+01,\n        1.40000000e+02, 5.00000000e+00, 0.00000000e+00, 6.00000000e+01,\n        1.40000000e+02, 2.00000000e+02, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 2.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        2.00000000e+00, 2.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n        1.00000000e+00, 3.00000000e+00, 3.00000000e+00, 4.00000000e+00,\n        1.00000000e+00, 1.00000000e+00, 2.28990981e+00, 2.40679360e+00,\n        2.00000000e+00]),\n -1)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.nanmean(x_train, axis=0)\n",
    "\n",
    "# Loop through each column\n",
    "for col in range(x_train.shape[1]):\n",
    "    nan_mask = np.isnan(x_train[:, col])\n",
    "    x_train[nan_mask, col] = mean[col]\n",
    "\n",
    "x_train.shape, y_train.shape, x_train[0,:], y_train[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:43.383537Z",
     "start_time": "2023-10-30T11:00:37.735462Z"
    }
   },
   "id": "3b9b8120cc5c994e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Classical Standardisation function\n",
    "    \n",
    "    Args:\n",
    "        x: Data in shape (N, D)\n",
    "        \n",
    "    Returns:\n",
    "        x: As standardised data\n",
    "        mean: Mean of each column\n",
    "        std: Standard deviation for each column\n",
    "    \"\"\"\n",
    "    mean = np.mean(x)\n",
    "    x = x - mean\n",
    "    std = np.std(x)\n",
    "    x = x/std\n",
    "    return x, mean, std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:43.443486Z",
     "start_time": "2023-10-30T11:00:43.377488Z"
    }
   },
   "id": "5350aad70ff33d3c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.07932676, -0.07932699, -0.04033713, -0.07932699, -0.07932691,\n       -0.07931432, -0.07932009, 12.6292844 , 12.6292844 , -0.07932702,\n       -0.07932702, -0.07932702, -0.07932702, -0.07932701, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932701, -0.07932702, -0.07932702, -0.07932702, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932647, -0.07932647, -0.07932667,\n       -0.07932702, -0.07932702, -0.07932701, -0.07932702, -0.07932701,\n       -0.07932702, -0.07932702, -0.079327  , -0.07932701, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932702, -0.07932701, -0.07932701, -0.07932701, -0.07932668,\n       -0.07932701, -0.07932702, -0.079327  , -0.07932702, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932701, -0.07932698, -0.07932647,\n       -0.0793264 , -0.07932701, -0.07932597, -0.07932384, -0.07932701,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932701, -0.07932701, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932698, -0.07932701, -0.07932142, -0.079327  , -0.0793266 ,\n       -0.07932699, -0.07932352, -0.07932352, -0.07932512, -0.07932639,\n       -0.07932573, -0.07932639, -0.07932702, -0.07932641, -0.07932636,\n       -0.07932576, -0.07932683, -0.07932639, -0.07932513, -0.07932576,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932701, -0.07932702,\n       -0.07932701, -0.07846534, -0.079327  , -0.07932701, -0.07932701,\n       -0.07687383, -0.079327  , -0.07932701, -0.07932701, -0.07932701,\n       -0.07932542, -0.07932533, -0.07932693, -0.07932693, -0.07932687,\n       -0.07932701, -0.07932701, -0.07932702, -0.07932701, -0.07932698,\n       -0.079327  , -0.07932701, -0.07932694, -0.07932701, -0.07932702,\n       -0.07932699, -0.07932701, -0.07932702, -0.07932701, -0.07932701,\n       -0.07932699, -0.07932701, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932701, -0.07932701, -0.079327  , -0.079327  , -0.07932701,\n       -0.079327  , -0.07932701, -0.07932702, -0.07932434, -0.07932701,\n       -0.07932663, -0.07932701, -0.07932664, -0.07932668, -0.07932679,\n       -0.07932273, -0.079327  , -0.07932699, -0.079327  , -0.07932699,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932702, -0.07932701, -0.07932702, -0.07932701, -0.07932701,\n       -0.07932701, -0.079327  , -0.07932701, -0.07932693, -0.07932701,\n       -0.07932702, -0.07932701, -0.07932702, -0.07932701, -0.079327  ,\n       -0.07932701, -0.07932701, -0.07932702, -0.07932701, -0.07932701,\n       -0.079327  , -0.07932702, -0.07932701, -0.079327  , -0.07932701,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932701, -0.07932701, -0.079327  , -0.079327  , -0.07932701,\n       -0.07932675, -0.07932701, -0.07932673, -0.07932702, -0.079327  ,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932702, -0.07932701,\n       -0.07932701, -0.07932662, -0.0793266 , -0.0793267 , -0.07932678,\n       -0.07932665, -0.07932656, -0.07932655, -0.07932653, -0.07932701,\n       -0.07932701, -0.0793269 , -0.07932702, -0.07932701, -0.07666523,\n       -0.07932568, -0.07932702, -0.07932568, -0.07932697, -0.07932698,\n       -0.07932698, -0.07932195, -0.07932697, -0.07932702, -0.07932367,\n       -0.07932702, -0.07932697, -0.07932702, -0.07932701, -0.07932702,\n       -0.07932702, -0.07932702, -0.07932701, -0.07932702, -0.07932702,\n       -0.07932702, -0.07932701, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932702, -0.07932696, -0.07932701, -0.07932661, -0.07932699,\n       -0.07932661, -0.07932701, -0.07932654, -0.07932685, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932701, -0.07932697, -0.07932702,\n       -0.07932701, -0.07932701, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932702, -0.07932702, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932702, -0.07932702, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932702, -0.07932702, -0.07932701, -0.07932701, -0.07932702,\n       -0.07932702, -0.07932702, -0.07932702, -0.07932702, -0.07932702,\n       -0.079327  , -0.07932699, -0.07932687, -0.079327  , -0.07932701,\n       -0.07932701, -0.07932627, -0.07932589, -0.07932699, -0.07932702,\n       -0.07932324, -0.07932589, -0.07932702, -0.07932702, -0.07931946,\n       -0.07932475, -0.07931719, -0.07932324, -0.07932589, -0.07932211,\n       -0.07932702, -0.07932702, -0.07932702, -0.07932702, -0.07932702,\n       -0.07932701, -0.07932701, -0.07932701, -0.07932701, -0.07932701,\n       -0.07932701, -0.07932702, -0.07932702, -0.07932701, -0.07932701,\n       -0.07932701])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, mean_x, std_x = standardize(x_train)\n",
    "x_train[3, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:47.843442Z",
     "start_time": "2023-10-30T11:00:43.413776Z"
    }
   },
   "id": "2d5b73b0b4c497f8"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "\n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "\n",
    "    >>> build_poly(np.array([0.0, 1.5]), 2)\n",
    "    array([[1.  , 0.  , 0.  ],\n",
    "           [1.  , 1.5 , 2.25]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    N, D = x.shape\n",
    "    poly = np.zeros((N, D * (degree + 1)))\n",
    "\n",
    "    for j in range(degree + 1):\n",
    "        poly[:, j * D:(j + 1) * D] = x ** j\n",
    "\n",
    "    return poly\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:47.899628Z",
     "start_time": "2023-10-30T11:00:47.845122Z"
    }
   },
   "id": "78bbd2808a3b7c61"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def build_model_data(x, y, degree=1):\n",
    "    poly = build_poly(x, degree)\n",
    "    return least_squares(y, poly)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T11:00:47.933336Z",
     "start_time": "2023-10-30T11:00:47.880202Z"
    }
   },
   "id": "4407139754917b5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0 MSE:  0.16100963017504696\n",
      "Degree:  1 MSE:  0.1603791082788717\n",
      "Degree:  2 MSE:  0.1609113576776\n",
      "Degree:  3 MSE:  0.1610047203334176\n",
      "Degree:  4 MSE:  0.1610059985398393\n",
      "Degree:  5 MSE:  0.16100938797939346\n",
      "Degree:  6 MSE:  0.1610093394873732\n",
      "Degree:  7 MSE:  0.1610092909981258\n",
      "Degree:  8 MSE:  0.16100924251165089\n"
     ]
    }
   ],
   "source": [
    "mes = []\n",
    "for i in range(10):\n",
    "    w, mse = build_model_data(x_train, y_train, i)\n",
    "    print(\"Degree: \", i, \"MSE: \", mse)\n",
    "    mes.append(mse)\n",
    "    \n",
    "plt.plot(mes)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-30T11:00:47.912601Z"
    }
   },
   "id": "3f42e13a77d92263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_class = np.expand_dims(y_train, axis=1)\n",
    "y_class[y_class == -1] = 0\n",
    "y_class"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a44d78a5c8abdc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression using gradient descent or SGD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd10abda45af484c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "\n",
    "    >>> sigmoid(np.array([0.1]))\n",
    "    array([0.52497919])\n",
    "    >>> sigmoid(np.array([0.1, 0.1]))\n",
    "    array([0.52497919, 0.52497919])\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f97d51058b3fed8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(4).reshape(2, 2)\n",
    "    >>> w = np.c_[[2., 3.]]\n",
    "    >>> round(calculate_loss(y, tx, w), 8)\n",
    "    1.52429481\n",
    "    \"\"\"\n",
    "    assert y.shape[0] == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    "\n",
    "    N = y.shape[0]\n",
    "    loss = (-1 / N) * np.sum(y*np.log(sigmoid(tx @ w)) + (1-y)*np.log(1-sigmoid(tx @ w)))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "542f3231ef96123d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "def calculate_gradient_sgd(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "    \"\"\"\n",
    "    index = random.randint(0, y.shape[0] - 1)\n",
    "    xn = tx[index, :]\n",
    "    yn = y[index, :]\n",
    "    return xn * (sigmoid(xn @ w) - yn)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> calculate_gradient(y, tx, w)\n",
    "    array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]])\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    return (1 / N) * tx.T @ (sigmoid(tx @ w)-y)\n",
    "\n",
    "def calculate_gradient_mb(y, tx, w, batch_size=30):\n",
    "    batch_indices = random.sample(range(y.shape[0]), batch_size)\n",
    "    X = tx[batch_indices]\n",
    "    Y = y[batch_indices]\n",
    "    N = len(batch_indices)\n",
    "    return (1/N) * X.T @ (sigmoid(X @ w)-Y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e8e6a4164331bf00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression. Return the loss and the updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: float\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> w\n",
    "    array([[0.11037076],\n",
    "           [0.17932896],\n",
    "           [0.24828716]])\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    return loss, w - gamma * gradient"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "35e337ff9f1878e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w=None, max_iter=10000, gamma=0.1, threshold=1e-12):\n",
    "    \"\"\"return the loss, gradient of the loss\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        initial_w:  shape=(D, 1)\n",
    "        max_iter: int and maximum amount of iterations\n",
    "        gamma: float and learning rate\n",
    "        threshold: float and threshold as absolute diff to stop\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> gradient, loss = logistic_regression(y, tx, w)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> gradient\n",
    "    (array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]]))\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w = np.zeros((tx.shape[1], 1)) if initial_w is None else initial_w\n",
    "    # Iterate over max iterations. Stop when we get convergence under threshold.\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        if (iter % 10) == 0:\n",
    "            print(\"Current iteration={%s}, loss={%s}\" % (iter, loss))\n",
    "        # converge check\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and (np.abs(losses[-1] - losses[-2]) < threshold):\n",
    "            break\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    print(\"loss={%s}\" % loss)\n",
    "    plt.plot(losses)\n",
    "    return w, loss"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "63818e03b7bd04b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = logistic_regression(y_class, x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8b5cb7a0d09ae23a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "64508fd9301374c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_x = np.array([[2.3, 3.2], [1.0, 0.1], [1.4, 2.3]])\n",
    "testing_y = np.array([0.1, 0.3, 0.5])\n",
    "testing_y = (testing_y > 0.2) * 1.0\n",
    "testing_init_w = np.array([0.5, 1.0])\n",
    "testing_res_w, testing_res_loss = logistic_regression(testing_y, testing_x, max_iter=2, gamma=0.1, initial_w=testing_init_w, threshold=1.0)\n",
    "\n",
    "expected_loss = 1.348358\n",
    "expected_w = np.array([0.378561, 0.801131])\n",
    "\n",
    "np.testing.assert_allclose(testing_res_loss, expected_loss, rtol=1e-4, atol=1e-8)\n",
    "np.testing.assert_allclose(testing_res_w, expected_w, rtol=1e-4, atol=1e-8)\n",
    "assert testing_res_loss.ndim == 0\n",
    "assert testing_res_w.shape == expected_w.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fa6817d290d7e8ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d6b4598d3715386"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
